{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from string import punctuation\n",
    "from typing import Any, Tuple\n",
    "from nltk.tokenize import casual_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHR_SPACE: str = \" \"\n",
    "CHR_APOST: str = \"창\\x80\\x99\"  # '\n",
    "CHR_SDQUOT: str = \"창\\x80\\x9c\" # \"\n",
    "CHR_DDQUOT: str = \"창\\x80\\x9d\" # \"\"\n",
    "CHR_MISC: str = \"창\\x80\\x94\"   # Not sure what this is but it gets replaced by a space\n",
    "\n",
    "def get_data(url: str) -> str:\n",
    "    text_output: str = requests.get(url=url).text\n",
    "    return text_output\n",
    "\n",
    "def clean_data(text_input: str) -> str:\n",
    "    index_start: int = text_input.index(\"One morning\")\n",
    "    index_end: int = text_input.rindex(\"*** END OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\")\n",
    "    text_output: str = (\n",
    "        text_input[index_start:index_end]\n",
    "        .replace(\"\\r\", CHR_SPACE)\n",
    "        .replace(\"\\n\", CHR_SPACE)\n",
    "        .replace(CHR_APOST, \"'\")\n",
    "        .replace(CHR_SDQUOT, '\"') # lol\n",
    "        .replace(CHR_DDQUOT, '\"') # The book replaces this char with \"\" so that thoughts and dialog plus \"he said\" etc. get captured as one sentence\n",
    "        .replace(CHR_MISC, CHR_SPACE)\n",
    "    )\n",
    "\n",
    "    return text_output\n",
    "\n",
    "def remove_char_from_entity(text_entity: str, char_to_remove: str) -> str:\n",
    "    if char_to_remove in text_entity:\n",
    "        start_index: int = text_entity.index(char_to_remove)\n",
    "        text_entity_cleaned: str = text_entity[:start_index]\n",
    "        return text_entity_cleaned\n",
    "\n",
    "    return text_entity\n",
    "\n",
    "def remove_punctuation(text_entity: str) -> str:\n",
    "    text_output: str = text_entity\n",
    "\n",
    "    for next_punc in punctuation:\n",
    "        text_output = remove_char_from_entity(text_entity=text_output, char_to_remove=next_punc)\n",
    "\n",
    "    return text_output\n",
    "\n",
    "def clean_entity(text_entity: str) -> str:\n",
    "    text_output: str = remove_char_from_entity(text_entity=text_entity, char_to_remove=\"'\")\n",
    "    text_output = remove_punctuation(text_entity=text_output)\n",
    "\n",
    "    return text_output\n",
    "\n",
    "def extract_entities(text_input: str, desired_tag: str) -> list[str]:\n",
    "    tokens = casual_tokenize(text=text_input)\n",
    "\n",
    "    # nltk.pos_tag gives pairs of token (0) + tag (1)\n",
    "    # Filtered on desired_tag\n",
    "    tags_output: list[str] = [clean_entity(row[0]) for row in nltk.pos_tag(tokens) if row[1] == desired_tag]\n",
    "    tags_output = [row for row in tags_output if len(row) > 1]\n",
    "\n",
    "    return tags_output\n",
    "\n",
    "def get_book_entities(url_book: str) -> list[str]:\n",
    "    # Step 1: Get book data\n",
    "    text_book: str = get_data(url=url_book)\n",
    "    text_cleaned: str = clean_data(text_input=text_book)\n",
    "\n",
    "    # Step 2: Get tags\n",
    "    sentences: list[str] = sent_tokenize(text=text_cleaned)\n",
    "\n",
    "    # Step 3: Get NNP tagged entities only, with punctuation etc. removed\n",
    "    entities: list[list[str]] = [extract_entities(text_input=next_sentence, desired_tag=\"NNP\") for next_sentence in sentences]\n",
    "    entities_cleaned: list[list[str]] = [x if len(x) > 0 else None for x in entities]    \n",
    "\n",
    "    # Step 4: Compile to dataframe\n",
    "    df_entities: pd.DataFrame = (\n",
    "        pd.DataFrame(dict(sentence=sentences, entities=entities_cleaned))\n",
    "        .dropna()\n",
    "    )\n",
    "    df_entities = (\n",
    "        df_entities[df_entities.entities.apply(len) > 1]\n",
    "    )\n",
    "\n",
    "    # Step 5: Just get a list\n",
    "    list_entities: list[str] = df_entities.entities.to_list()\n",
    "    \n",
    "    return list_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_metamorphosis: str = \"https://www.gutenberg.org/files/5200/5200-0.txt\"\n",
    "entities_metamorphosis: list[str] = get_book_entities(url_book=url_metamorphosis)\n",
    "\n",
    "entities_metamorphosis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
