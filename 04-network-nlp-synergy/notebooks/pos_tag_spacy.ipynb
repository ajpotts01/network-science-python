{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import spacy\n",
    "from spacy import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp: Language = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHR_SPACE: str = \" \"\n",
    "CHR_APOST: str = \"창\\x80\\x99\"  # '\n",
    "CHR_SDQUOT: str = \"창\\x80\\x9c\" # \"\n",
    "CHR_DDQUOT: str = \"창\\x80\\x9d\" # \"\"\n",
    "CHR_MISC: str = \"창\\x80\\x94\"   # Not sure what this is but it gets replaced by a space\n",
    "\n",
    "def get_data(url: str) -> str:\n",
    "    text_output: str = requests.get(url=url).text\n",
    "    return text_output\n",
    "\n",
    "def clean_data(text_input: str) -> str:\n",
    "    index_start: int = text_input.index(\"One morning\")\n",
    "    index_end: int = text_input.rindex(\"*** END OF THE PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\")\n",
    "    text_output: str = (\n",
    "        text_input[index_start:index_end]\n",
    "        .replace(\"\\r\", CHR_SPACE)\n",
    "        .replace(\"\\n\", CHR_SPACE)\n",
    "        .replace(CHR_APOST, \"'\")\n",
    "        .replace(CHR_SDQUOT, '\"') # lol\n",
    "        .replace(CHR_DDQUOT, '\"') # The book replaces this char with \"\" so that thoughts and dialog plus \"he said\" etc. get captured as one sentence\n",
    "        .replace(CHR_MISC, CHR_SPACE)\n",
    "    )\n",
    "\n",
    "    return text_output\n",
    "\n",
    "def remove_char_from_entity(text_entity: str, char_to_remove: str) -> str:\n",
    "    if char_to_remove in text_entity:\n",
    "        start_index: int = text_entity.index(char_to_remove)\n",
    "        text_entity_cleaned: str = text_entity[:start_index]\n",
    "        return text_entity_cleaned\n",
    "\n",
    "    return text_entity\n",
    "\n",
    "def clean_entity(text_entity: str) -> str:\n",
    "    text_output: str = text_entity.strip()\n",
    "    text_output = remove_char_from_entity(text_entity=text_output, char_to_remove=\"'s\")\n",
    "\n",
    "    return text_output\n",
    "\n",
    "def extract_entities_from_sentence(sentence: spacy.tokens.span.Span, desired_tags: list[str]) -> list[str]:\n",
    "    sentence_doc = nlp(sentence.text)\n",
    "\n",
    "    entities: list[str] = [clean_entity(next_entity.text) for next_entity in sentence_doc.ents if next_entity.label_ in desired_tags]\n",
    "    entities = list(filter(lambda x: x != \"\", entities))\n",
    "\n",
    "    return list(set(entities))\n",
    "\n",
    "def extract_entities(sentences_input: list[str], desired_tags: list[str]) -> list[str]:\n",
    "    # Filtered on desired_tag\n",
    "    \n",
    "    entities: list[str] = [extract_entities_from_sentence(sentence=next_sentence, desired_tags=desired_tags) for next_sentence in sentences_input]\n",
    "    \n",
    "    entities = list(filter(lambda x: len(x) > 1, entities))\n",
    "\n",
    "    return entities\n",
    "\n",
    "def get_book_entities(url_book: str) -> list[str]:\n",
    "    # Step 1: Get book data\n",
    "    text_book: str = get_data(url=url_book)\n",
    "    text_cleaned: str = clean_data(text_input=text_book)\n",
    "\n",
    "    doc = nlp(text=text_cleaned)\n",
    "\n",
    "    # Step 2: Get tags\n",
    "    sentences: list[str] = list(doc.sents)\n",
    "\n",
    "    entities: list[list[str]] = extract_entities(sentences_input=sentences, desired_tags=[\"PERSON\", \"ORG\", \"GPE\"])\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_metamorphosis: str = \"https://www.gutenberg.org/files/5200/5200-0.txt\"\n",
    "entities_metamorphosis: list[list[str]] = get_book_entities(url_book=url_metamorphosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Gregor', 'Grete'],\n",
       " ['Gregor', 'Grete'],\n",
       " ['fro', 'Gregor'],\n",
       " ['Gregor', 'Grete'],\n",
       " ['Gregor', 'Grete'],\n",
       " ['Gregor', 'Grete'],\n",
       " ['Gregor', 'Grete'],\n",
       " ['Gregor', 'Grete'],\n",
       " ['jaws', 'Gregor'],\n",
       " ['Gregor', 'Samsa'],\n",
       " ['Gregor', 'Samsa'],\n",
       " ['Samsa', 'Grete'],\n",
       " ['Samsa', 'Grete'],\n",
       " ['Samsa', 'Grete']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_metamorphosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_dk(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    entities = []\n",
    "    for sentence in sentences:\n",
    "        sentence_entities = []\n",
    "        sent_doc = nlp(sentence.text)\n",
    "        for ent in sent_doc.ents:\n",
    "            if ent.label_ in ['PERSON', 'ORG', 'GPE']:\n",
    "                entity = ent.text.strip()\n",
    "                if \"'s\" in entity:\n",
    "                    cutoff = entity.index(\"'s\")\n",
    "                    entity = entity[:cutoff]\n",
    "                if entity != '':\n",
    "                    sentence_entities.append(entity)\n",
    "        sentence_entities = list(set(sentence_entities))\n",
    "        if len(sentence_entities) > 1:\n",
    "            entities.append(sentence_entities)\n",
    "    return entities\n",
    "\n",
    "def get_book_entities_dk(url_book: str) -> list[str]:\n",
    "    # Step 1: Get book data\n",
    "    text_book: str = get_data(url=url_book)\n",
    "    text_cleaned: str = clean_data(text_input=text_book)\n",
    "\n",
    "    entities: list[list[str]] = extract_entities_dk(text=text_cleaned)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check against the book - the book lists different output to what the copy-pasted code does, but same as the code written above\n",
    "# This is likely a difference in spaCy lib/model versions.\n",
    "entities_dk: list[list[str]] = get_book_entities_dk(url_book=url_metamorphosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Gregor', 'Grete'],\n",
       " ['Gregor', 'Grete'],\n",
       " ['fro', 'Gregor'],\n",
       " ['Gregor', 'Grete'],\n",
       " ['Gregor', 'Grete'],\n",
       " ['Gregor', 'Grete'],\n",
       " ['Gregor', 'Grete'],\n",
       " ['Gregor', 'Grete'],\n",
       " ['jaws', 'Gregor'],\n",
       " ['Gregor', 'Samsa'],\n",
       " ['Gregor', 'Samsa'],\n",
       " ['Samsa', 'Grete'],\n",
       " ['Samsa', 'Grete'],\n",
       " ['Samsa', 'Grete']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_dk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for next_index in range(len(entities_metamorphosis)):\n",
    "    print(f\"{entities_metamorphosis[next_index]=}\")\n",
    "    print(f\"{entities_dk[next_index]=}\")\n",
    "    print(f\"Match: {entities_dk[next_index] == entities_metamorphosis[next_index]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
